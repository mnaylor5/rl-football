{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Torchbeast model for submission\n",
    "Need to unravel the code necessary to pass pixel obs into a buffer/model, so that I can submit an agent trained with `torchbeast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import gfootball.env as football_env\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/home/jupyter/logs/torchbeast/empty_goal/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easy part: the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariNet(nn.Module):\n",
    "    def __init__(self, observation_shape, num_actions, use_lstm=False):\n",
    "        super(AtariNet, self).__init__()\n",
    "        self.observation_shape = observation_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Feature extraction.\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=self.observation_shape[0],\n",
    "            out_channels=32,\n",
    "            kernel_size=8,\n",
    "            stride=4,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Fully connected layer.\n",
    "        self.fc = nn.Linear(3136, 512)\n",
    "\n",
    "        # FC output size + one-hot of last action + last reward.\n",
    "        core_output_size = self.fc.out_features + num_actions + 1\n",
    "\n",
    "        self.use_lstm = use_lstm\n",
    "        if use_lstm:\n",
    "            self.core = nn.LSTM(core_output_size, core_output_size, 2)\n",
    "\n",
    "        self.policy = nn.Linear(core_output_size, self.num_actions)\n",
    "        self.baseline = nn.Linear(core_output_size, 1)\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        if not self.use_lstm:\n",
    "            return tuple()\n",
    "        return tuple(\n",
    "            torch.zeros(self.core.num_layers, batch_size, self.core.hidden_size)\n",
    "            for _ in range(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, core_state=()):\n",
    "        x = inputs[\"frame\"]  # [T, B, C, H, W].\n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.\n",
    "        x = x.float() / 255.0\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(T * B, -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "\n",
    "        one_hot_last_action = F.one_hot(\n",
    "            inputs[\"last_action\"].view(T * B), self.num_actions\n",
    "        ).float()\n",
    "        clipped_reward = torch.clamp(inputs[\"reward\"], -1, 1).view(T * B, 1)\n",
    "        core_input = torch.cat([x, clipped_reward, one_hot_last_action], dim=-1)\n",
    "\n",
    "        if self.use_lstm:\n",
    "            core_input = core_input.view(T, B, -1)\n",
    "            core_output_list = []\n",
    "            notdone = (~inputs[\"done\"]).float()\n",
    "            for input, nd in zip(core_input.unbind(), notdone.unbind()):\n",
    "                # Reset core state to zero whenever an episode ended.\n",
    "                # Make `done` broadcastable with (num_layers, B, hidden_size)\n",
    "                # states:\n",
    "                nd = nd.view(1, -1, 1)\n",
    "                core_state = tuple(nd * s for s in core_state)\n",
    "                output, core_state = self.core(input.unsqueeze(0), core_state)\n",
    "                core_output_list.append(output)\n",
    "            core_output = torch.flatten(torch.cat(core_output_list), 0, 1)\n",
    "        else:\n",
    "            core_output = core_input\n",
    "            core_state = tuple()\n",
    "\n",
    "        policy_logits = self.policy(core_output)\n",
    "        baseline = self.baseline(core_output)\n",
    "\n",
    "        if self.training:\n",
    "            action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        else:\n",
    "            # Don't sample when testing.\n",
    "            action = torch.argmax(policy_logits, dim=1)\n",
    "\n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        baseline = baseline.view(T, B)\n",
    "        action = action.view(T, B)\n",
    "\n",
    "        return (\n",
    "            dict(policy_logits=policy_logits, baseline=baseline, action=action),\n",
    "            core_state,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framestack is what's making this have 16 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AtariNet((16, 84, 84), num_actions=19, use_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(MODEL_PATH, 'model.tar'), map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observation processing\n",
    "* scale=True\n",
    "* frame_stack=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_frame(obs):\n",
    "    return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_frame(frame):\n",
    "    frame = torch.from_numpy(frame)\n",
    "    return frame.view((1, 1) + frame.shape)  # (...) -> (T,B,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = deque([], maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = football_env.create_environment('11_vs_11_kaggle', channel_dimensions=(84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_obs = torch.from_numpy(scale_frame(raw_obs.transpose(2, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 84, 84])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5): frames.append(clean_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obs = torch.cat(list(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 84, 84])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = model.initial_state(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_in = {\n",
    "    'frame':model_obs.unsqueeze(0).unsqueeze(0),\n",
    "    'last_action':torch.tensor(1),\n",
    "    'reward':torch.tensor(0).view(1, 1),\n",
    "    'done':torch.tensor(False).view(1, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_output, unused_state = model(model_in, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(agent_output['action'].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put it into a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = football_env.create_environment('academy_empty_goal_close', channel_dimensions=(84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtariNet(\n",
       "  (conv1): Conv2d(16, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (core): LSTM(532, 532, num_layers=2)\n",
       "  (policy): Linear(in_features=532, out_features=19, bias=True)\n",
       "  (baseline): Linear(in_features=532, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 100\n",
    "rewards = []\n",
    "for _ in range(EPISODES):\n",
    "    frames = deque([], maxlen=4)\n",
    "    done = False\n",
    "    raw_obs = base_env.reset()\n",
    "    last_action = 0\n",
    "    reward = 0\n",
    "    agent_state = model.initial_state(1)\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # prepare observation \n",
    "        clean_obs = torch.from_numpy(scale_frame(raw_obs.transpose(2, 0, 1)))\n",
    "        frames.append(clean_obs)\n",
    "\n",
    "        # get actions\n",
    "        if len(frames) < 4:\n",
    "            action = 1\n",
    "        else:\n",
    "            model_obs = torch.cat(list(frames)).unsqueeze(0).unsqueeze(0)\n",
    "            model_in = {\n",
    "                'frame':model_obs,\n",
    "                'last_action':torch.tensor(last_action),\n",
    "                'reward':torch.tensor(reward).view(1, 1),\n",
    "                'done':torch.tensor(done).view(1, 1)\n",
    "            }\n",
    "            action = model(model_in, agent_state)[0]['action'].item()\n",
    "\n",
    "        raw_obs, reward, done, _ = base_env.step(action)\n",
    "        last_action = action\n",
    "        episode_reward += reward\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m56"
  },
  "kernelspec": {
   "display_name": "football",
   "language": "python",
   "name": "football"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Torchbeast model for submission\n",
    "Need to unravel the code necessary to pass pixel obs into a buffer/model, so that I can submit an agent trained with `torchbeast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import gfootball.env as football_env\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/home/jupyter/logs/torchbeast/empty_goal/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easy part: the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariNet(nn.Module):\n",
    "    def __init__(self, observation_shape, num_actions, use_lstm=False):\n",
    "        super(AtariNet, self).__init__()\n",
    "        self.observation_shape = observation_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Feature extraction.\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=self.observation_shape[0],\n",
    "            out_channels=32,\n",
    "            kernel_size=8,\n",
    "            stride=4,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Fully connected layer.\n",
    "        self.fc = nn.Linear(3136, 512)\n",
    "\n",
    "        # FC output size + one-hot of last action + last reward.\n",
    "        core_output_size = self.fc.out_features + num_actions + 1\n",
    "\n",
    "        self.use_lstm = use_lstm\n",
    "        if use_lstm:\n",
    "            self.core = nn.LSTM(core_output_size, core_output_size, 2)\n",
    "\n",
    "        self.policy = nn.Linear(core_output_size, self.num_actions)\n",
    "        self.baseline = nn.Linear(core_output_size, 1)\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        if not self.use_lstm:\n",
    "            return tuple()\n",
    "        return tuple(\n",
    "            torch.zeros(self.core.num_layers, batch_size, self.core.hidden_size)\n",
    "            for _ in range(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, core_state=()):\n",
    "        x = inputs[\"frame\"]  # [T, B, C, H, W].\n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.\n",
    "        x = x.float() / 255.0\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(T * B, -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "\n",
    "        one_hot_last_action = F.one_hot(\n",
    "            inputs[\"last_action\"].view(T * B), self.num_actions\n",
    "        ).float()\n",
    "        clipped_reward = torch.clamp(inputs[\"reward\"], -1, 1).view(T * B, 1)\n",
    "        core_input = torch.cat([x, clipped_reward, one_hot_last_action], dim=-1)\n",
    "\n",
    "        if self.use_lstm:\n",
    "            core_input = core_input.view(T, B, -1)\n",
    "            core_output_list = []\n",
    "            notdone = (~inputs[\"done\"]).float()\n",
    "            for input, nd in zip(core_input.unbind(), notdone.unbind()):\n",
    "                # Reset core state to zero whenever an episode ended.\n",
    "                # Make `done` broadcastable with (num_layers, B, hidden_size)\n",
    "                # states:\n",
    "                nd = nd.view(1, -1, 1)\n",
    "                core_state = tuple(nd * s for s in core_state)\n",
    "                output, core_state = self.core(input.unsqueeze(0), core_state)\n",
    "                core_output_list.append(output)\n",
    "            core_output = torch.flatten(torch.cat(core_output_list), 0, 1)\n",
    "        else:\n",
    "            core_output = core_input\n",
    "            core_state = tuple()\n",
    "\n",
    "        policy_logits = self.policy(core_output)\n",
    "        baseline = self.baseline(core_output)\n",
    "\n",
    "        if self.training:\n",
    "            action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        else:\n",
    "            # Don't sample when testing.\n",
    "            action = torch.argmax(policy_logits, dim=1)\n",
    "\n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        baseline = baseline.view(T, B)\n",
    "        action = action.view(T, B)\n",
    "\n",
    "        return (\n",
    "            dict(policy_logits=policy_logits, baseline=baseline, action=action),\n",
    "            core_state,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framestack is what's making this have 16 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AtariNet((16, 84, 84), num_actions=19, use_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(MODEL_PATH, 'model.tar'), map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observation processing\n",
    "* scale=True\n",
    "* frame_stack=True\n",
    "\n",
    "Repurposing my old history buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my history buffer class\n",
    "class HistoryBuffer():\n",
    "    def __init__(self, n, obs_shape=(4, 84, 84), fill_value=0., device=None):\n",
    "        '''\n",
    "        Set up the history buffer.\n",
    "\n",
    "        Inputs: \n",
    "          - n: number of observations to retain\n",
    "          - obs_shape: tuple representing the shape of a single normalize observation\n",
    "          - fill_value: value used to fill initial buffer tensors\n",
    "        '''\n",
    "        self.n=n\n",
    "        self.obs_shape=obs_shape\n",
    "        self.fill_value=fill_value \n",
    "        self.device=device\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = deque(maxlen=self.n)\n",
    "        for _ in range(self.n):\n",
    "            self.buffer.append(torch.full(self.obs_shape, fill_value=self.fill_value))\n",
    "    \n",
    "    def append(self, obs):\n",
    "        'Normalize a raw observation and append to the history buffer'\n",
    "        norm = self.normalize_obs(obs)\n",
    "        self.buffer.append(norm)\n",
    "\n",
    "    def get_tensor(self):\n",
    "        '''\n",
    "        Return a single tensor containing the observations in the buffer.\n",
    "        Uses torch.stack on the torch.Tensors within the deque; most recent \n",
    "        observations will be at the end of the first index. \n",
    "\n",
    "        Returns: (Sequence x Channels x Pitch Length x Pitch Width)\n",
    "        '''\n",
    "        if self.device is not None:\n",
    "            return torch.cat(list(self.buffer)).to(self.device)\n",
    "        else:\n",
    "            return torch.cat(list(self.buffer))\n",
    "\n",
    "    def normalize_obs(self, obs):\n",
    "        'Return the normalized pixel observation in the shape (Channels x Length x Width)'\n",
    "        return torch.from_numpy((obs/255).transpose(2, 0, 1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = football_env.create_environment('11_vs_11_kaggle', channel_dimensions=(84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff = HistoryBuffer(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5): buff.append(raw_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obs = buff.get_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_in = {\n",
    "    'frame':model_obs.unsqueeze(0).unsqueeze(0),\n",
    "    'last_action':torch.tensor(1),\n",
    "    'reward':torch.tensor(0).view(1, 1),\n",
    "    'done':torch.tensor(False).view(1, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = model.initial_state(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'policy_logits': tensor([[[-0.1676,  0.4421,  0.3763, -0.1808,  0.7571,  3.9786,  0.8066,\n",
       "             0.2107, -0.2174, -1.9995, -1.9141, -2.4971, -0.2875, -0.8430,\n",
       "            -0.0868,  0.4746, -0.9659, -0.4161,  1.1495]]],\n",
       "         grad_fn=<ViewBackward>),\n",
       "  'baseline': tensor([[0.9666]], grad_fn=<ViewBackward>),\n",
       "  'action': tensor([[5]])},\n",
       " (tensor([[[-0.3732, -0.0995, -0.0600,  ...,  0.2822,  0.4375,  0.3881]],\n",
       "  \n",
       "          [[-0.2363, -0.1951,  0.0311,  ...,  0.2163,  0.0717, -0.2254]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.6079, -0.1861, -0.1225,  ...,  0.4782,  0.6633,  0.6863]],\n",
       "  \n",
       "          [[-0.4371, -0.4068,  0.0704,  ...,  0.4171,  0.1392, -0.3970]]],\n",
       "         grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(model_in, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_output, unused_state = model(model_in, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(agent_output['action'].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put it into a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = football_env.create_environment('academy_empty_goal', channel_dimensions=(84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtariNet(\n",
       "  (conv1): Conv2d(16, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (core): LSTM(532, 532, num_layers=2)\n",
       "  (policy): Linear(in_features=532, out_features=19, bias=True)\n",
       "  (baseline): Linear(in_features=532, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 84, 84])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff = HistoryBuffer(4)\n",
    "torch.cat(list(buff.buffer)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 25\n",
    "rewards = []\n",
    "for _ in range(EPISODES):\n",
    "    done = False\n",
    "    obs = base_env.reset()\n",
    "    last_action = 0\n",
    "    reward = 0\n",
    "    agent_state = model.initial_state(1)\n",
    "    episode_reward = 0\n",
    "    buff = HistoryBuffer(4)\n",
    "\n",
    "    while not done:\n",
    "        # prepare observation \n",
    "        buff.append(obs)\n",
    "        clean_obs = buff.get_tensor().unsqueeze(0).unsqueeze(0)\n",
    "        model_in = {\n",
    "            'frame':clean_obs,\n",
    "            'last_action':torch.tensor(last_action),\n",
    "            'reward':torch.tensor(0).view(1, 1),\n",
    "            'done':torch.tensor(False).view(1, 1)\n",
    "        }\n",
    "        action = model(model_in, agent_state)[0]['action'].item()\n",
    "\n",
    "        obs, reward, done, _ = base_env.step(action)\n",
    "        last_action = action\n",
    "        episode_reward += reward\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw environment - get rewards when we can't call `.step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_env = football_env.create_environment('academy_empty_goal', channel_dimensions=(84, 84), representation='raw')\n",
    "raw_obs = raw_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_score, their_score = raw_obs[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_frame(obs):\n",
    "    return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = deque([], maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_obs = torch.from_numpy(scale_frame(raw_obs.transpose(2, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 84, 84])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5): frames.append(clean_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obs = torch.cat(list(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 84, 84])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = model.initial_state(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_in = {\n",
    "    'frame':model_obs.unsqueeze(0).unsqueeze(0),\n",
    "    'last_action':torch.tensor(1),\n",
    "    'reward':torch.tensor(0).view(1, 1),\n",
    "    'done':torch.tensor(False).view(1, 1)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m56"
  },
  "kernelspec": {
   "display_name": "football",
   "language": "python",
   "name": "football"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

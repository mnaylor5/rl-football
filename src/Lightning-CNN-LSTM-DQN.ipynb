{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Training Loop: CNN-LSTM DQN\n",
    "Developed first in a colab notebook, based on [this Medium post](https://towardsdatascience.com/en-lightning-reinforcement-learning-a155c217c3de) (which is also integrated into the Lightning repo examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gfootball.env as football_env\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from collections import deque, OrderedDict\n",
    "import random \n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from dqn_utils import HistoryBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "# encoder for pixel images\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.c1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=1)\n",
    "        self.c2 = nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=1)\n",
    "        self.c3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.c4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.linear = nn.Linear(1536, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.c1(x))\n",
    "        h = self.relu(self.c2(h))\n",
    "        h = self.relu(self.c3(h))\n",
    "        h = self.relu(self.c4(h))\n",
    "        flattened = h.flatten(-3)\n",
    "        out = self.relu(self.linear(flattened))\n",
    "        return out\n",
    "\n",
    "# agent class\n",
    "class HistoryConvAgent(nn.Module):\n",
    "    def __init__(self, dropout_p = 0.1, action_size=18):\n",
    "        super(HistoryConvAgent, self).__init__()\n",
    "        self.encoder = CNNEncoder(out_size=256)\n",
    "        self.gru = nn.GRU(256, 256, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, action_size)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # batching doesn't play nicely here\n",
    "        if x.ndim == 4:\n",
    "            encoded = self.encoder(x)\n",
    "            _, gru_out = self.gru(encoded.unsqueeze(0))\n",
    "        else:\n",
    "            encoded = torch.stack([self.encoder(x[i]) for i in range(x.shape[0])])\n",
    "            _, gru_out = self.gru(encoded)\n",
    "        gru_out = self.dropout(gru_out.squeeze())\n",
    "        fc1_out = self.activation(self.fc1(gru_out))\n",
    "        fc2_out = self.activation(self.fc2(fc1_out))\n",
    "        return fc2_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the replay buffer its own class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, dones, next_states = zip(*batch)\n",
    "        return states, actions, rewards, dones, next_states\n",
    "\n",
    "class RLDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, buffer, sample_size=64):\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "  \n",
    "    def __iter__(self):\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic environment getter - including the checkpoint rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(scenario='11_vs_11_kaggle'):\n",
    "    env = football_env.create_environment(env_name=scenario, \n",
    "                                      stacked=False, \n",
    "                                      representation='extracted',\n",
    "                                      write_goal_dumps=False, \n",
    "                                      write_full_episode_dumps=False, \n",
    "                                      rewards='scoring,checkpoints',\n",
    "                                      render=False)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class to interact with the environment: handle buffer creation, stepping, resetting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    Base class for interacting with environment\n",
    "    '''\n",
    "    def __init__(self, env, replay_buffer, history_length=5):\n",
    "        self.env = env \n",
    "        self.replay_buffer = replay_buffer \n",
    "        self.history_length = history_length\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        self.history_buffer = HistoryBuffer(self.history_length)\n",
    "        self.history_buffer.append(obs)\n",
    "        self.state = self.history_buffer.get_tensor()\n",
    "\n",
    "    def get_action(self, net, epsilon, device):\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, 17)\n",
    "        else:\n",
    "            state = self.state\n",
    "            if device != 'cpu':\n",
    "                state = state.cuda(device)\n",
    "            q_values = net(state)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "        return int(action)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net, epsilon=0.0, device='cpu'):\n",
    "        action = self.get_action(net, epsilon, device)\n",
    "        obs, reward, done, _ = self.env.step(action)\n",
    "        self.history_buffer.append(obs)\n",
    "        next_state = self.history_buffer.get_tensor()\n",
    "        exp = (self.state, action, reward, done, next_state)\n",
    "        self.replay_buffer.append(exp)\n",
    "\n",
    "        self.state = next_state \n",
    "        if done:\n",
    "            self.reset()\n",
    "\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'scenario':'11_vs_11_kaggle',\n",
    "    'total_steps':1_000_000,\n",
    "    'eps_start':1.0, # beginning epsilon value\n",
    "    'eps_end':0.01, # final epsilon value\n",
    "    'eps_last_frame':600_000, # training step to stop decaying epsilon\n",
    "    'gamma':0.99,\n",
    "    'replay_size':256,\n",
    "    'history_len':5,\n",
    "    'sync_rate':150,\n",
    "    'warm_start_steps':1_000,\n",
    "    'lr':1e-4,\n",
    "    'batch_size':24\n",
    "}\n",
    "\n",
    "class AgentLightning(pl.LightningModule):\n",
    "    def __init__(self, hparams:dict):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.env = get_env(self.hparams['scenario'])\n",
    "        self.replay_buffer = ReplayBuffer(self.hparams['replay_size'])\n",
    "        self.net = HistoryConvAgent()\n",
    "        self.target_net = deepcopy(self.net)\n",
    "        self.agent = Agent(self.env, self.replay_buffer, self.hparams['history_len'])\n",
    "        self.total_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.rolling_rewards = deque(maxlen=100)\n",
    "        self.populate(self.hparams['warm_start_steps'])\n",
    "        self.episodes_total = 0\n",
    "\n",
    "    def populate(self, steps=1000):\n",
    "        'Warm up with random moves to populate the replay buffer'\n",
    "        for _ in range(steps):\n",
    "            self.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "    def dqn_mse_loss(self, batch):\n",
    "        states, actions, rewards, dones, next_states = batch \n",
    "        state_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "    \n",
    "        expected_state_action_values = next_state_values * self.hparams['gamma'] + rewards\n",
    "\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.hparams['lr'])\n",
    "        return [optimizer]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDataset(self.replay_buffer, self.hparams['batch_size'])\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, self.hparams['batch_size'], sampler=None)\n",
    "        return dataloader\n",
    "\n",
    "    def get_device(self, batch):\n",
    "        return batch[0].device.index if self.on_gpu else 'cpu'\n",
    "\n",
    "    def training_step(self, batch, nb_batch):\n",
    "        device = self.get_device(batch)\n",
    "        epsilon = max(self.hparams['eps_end'], self.hparams['eps_start'] * (1 - self.global_step/self.hparams['eps_last_frame']))\n",
    "\n",
    "        reward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "\n",
    "        self.episode_reward += reward \n",
    "\n",
    "        loss = self.dqn_mse_loss(batch)\n",
    "\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss = loss.unsqueeze(0)\n",
    "\n",
    "        if done:\n",
    "            self.total_reward += self.episode_reward \n",
    "            self.rolling_rewards.append(self.episode_reward)\n",
    "            self.episode_reward = 0\n",
    "            self.episodes_total += 1\n",
    "    \n",
    "        if self.global_step % self.hparams['sync_rate'] == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "    \n",
    "        if len(self.rolling_rewards) > 30:\n",
    "            avg_reward = np.mean(self.rolling_rewards)\n",
    "        else:\n",
    "            avg_reward = 0\n",
    "            \n",
    "        log = {\n",
    "            'total_reward':torch.tensor(self.total_reward).to(device),\n",
    "            'episodes_total':self.episodes_total,\n",
    "            'average_reward': avg_reward,\n",
    "            'epsilon':epsilon\n",
    "        }\n",
    "        self.log_dict(log, prog_bar=True, logger=True, on_epoch=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgentLightning(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "INFO:lightning:Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, \n",
    "                     max_epochs=1_000_000, \n",
    "                     val_check_interval=10_000, \n",
    "                     precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | net        | HistoryConvAgent | 991 K \n",
      "1 | target_net | HistoryConvAgent | 991 K \n",
      "INFO:lightning:\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | net        | HistoryConvAgent | 991 K \n",
      "1 | target_net | HistoryConvAgent | 991 K \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111837ad04d64ba995b33da9ab59f1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process of playing a match and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr = 0\n",
    "# env = get_env('11_vs_11_hard_stochastic')\n",
    "# obs = env.reset()\n",
    "# history = HistoryBuffer(5)\n",
    "# history.append(obs)\n",
    "# done = False\n",
    "# with torch.no_grad():\n",
    "#     while not done:\n",
    "#         q = model.net(history.get_tensor())\n",
    "#         action = int(q.argmax().item())\n",
    "#         obs, reward, done, _ = env.step(action)\n",
    "#         history.append(obs)\n",
    "#         cr += reward"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m56"
  },
  "kernelspec": {
   "display_name": "football",
   "language": "python",
   "name": "football"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
